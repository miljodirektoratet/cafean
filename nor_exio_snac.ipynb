{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c5be9c",
   "metadata": {},
   "source": [
    " # Hybrid SNAC model adopted for Norway SSB data\n",
    "\n",
    "Top level script for calculation Norwegian carbon footprints\n",
    "based on the hybrid-SNAC approach, coupling EXIOBASE 3\n",
    "with official data from SSB Norway\n",
    "\n",
    "Copyright (C) 2023, XIO Sustainability Analytics, Inc - All Rights Reserved\n",
    "\n",
    "Unauthorized copying of this file, via any medium is strictly prohibited\n",
    "Proprietary and confidential\n",
    "\n",
    "Written by\n",
    "\n",
    "    - Richard Wood\n",
    "    - Konstantin Stadler\n",
    "\n",
    "\n",
    "You must set the directory for the EXIOBASE data with\n",
    "exio3_folder, or you can uncomment text to use the autodownload function\n",
    "\n",
    "Requires pandas, numpy and pymrio to run.\n",
    "Pymrio available at: https://github.com/IndEcol/pymrio/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe65bb",
   "metadata": {},
   "source": [
    "Import of required external packages. These are all available through pip/conda/mamba install."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ee27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import openpyxl as opx\n",
    "\n",
    "import pandas as pd\n",
    "import pymrio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf2b6a9",
   "metadata": {},
   "source": [
    "Python internal packages (this dont need to be installed, part of standard python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889ca977",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137a5976",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Run the model for the following years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c6eee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# range command is exclusive of the last number, so we add 1\n",
    "years = list(range(2012, 2021))\n",
    "# for only running one year we can do\n",
    "# years: list[int] = [2020]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9979af",
   "metadata": {},
   "source": [
    "### Path definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf24e912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the work path to the directory where this script is located\n",
    "# and if this is not available to the current working directory\n",
    "try:\n",
    "    work_path = Path(__file__).parent.absolute()  # when running as script\n",
    "except NameError:\n",
    "    work_path = Path.cwd()\n",
    "\n",
    "data_path: Path = work_path / \"data\"\n",
    "\n",
    "output_path: Path = work_path / \"results\"\n",
    "output_path.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0643da",
   "metadata": {},
   "source": [
    "## Specifying/reading data sources\n",
    "This section here defines all data sources needed for the calculation.\n",
    "Smaller dataset are read already here, larger datasets are read\n",
    "at the places they are needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f7167d",
   "metadata": {},
   "source": [
    "The sector matching file provides the bridge from the\n",
    "EXIOBASE sector classification to the Norwegian official\n",
    "classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a1ac04",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_matching = pd.read_excel(\n",
    "    data_path / \"sector_matching.xlsx\",\n",
    "    sheet_name=\"EXIOBASEtoNO65\",\n",
    "    index_col=[1, 2, 3],\n",
    "    header=[1, 2, 3],\n",
    ").iloc[\n",
    "    :, 1:\n",
    "]  # get rid of the summation column\n",
    "sector_matching.index.names = [\"nr\", \"code\", \"name\"]\n",
    "sector_matching.columns.names = [\"nr\", \"name\", \"code\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c02af",
   "metadata": {},
   "source": [
    "The sector list file contains the names used for\n",
    "parsing the sectors in the emission data. It is also\n",
    "used for then renaming the sectors to the names\n",
    "used in the official economic data.\n",
    "*NOTE:* The official economic data is provided in\n",
    "industry by industry (ixi) format, but the names\n",
    "are \"product names\". We stay consistent to that\n",
    "and also use product names throughout the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1759566d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sector_list = pd.read_excel(data_path / \"sector_list.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a44c5c9f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "The emission characterization file contains\n",
    "  1. The conversion factors from EXIOBASE emission units/names to Norwegian emission units/names\n",
    "  2. The characterization of GHG to GWP100\n",
    "*NOTE: *: All unit conversion for emission data happens in that file!\n",
    "These units must match the units provided by the EXIOBASE and Norwegian emission data.\n",
    "Double-check these when updating the data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c57305",
   "metadata": {},
   "outputs": [],
   "source": [
    "exio_stressor_to_nor = pd.read_excel(\n",
    "    data_path / \"emis_conv_char.xlsx\", sheet_name=\"exio_nor_conv\", index_col=0\n",
    ")\n",
    "charact_ghg = pd.read_excel(\n",
    "    data_path / \"emis_conv_char.xlsx\", sheet_name=\"nor_char\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4502d647",
   "metadata": {},
   "source": [
    "The official Norwegian emission data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78435324",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_emission_file = (\n",
    "    data_path / \"AEA Questionnaire_2023_Norway2023_til footprint.xlsm\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3c5d31",
   "metadata": {},
   "source": [
    "Get the exchange rates per year.\n",
    "These are currently manually inserted from Eurostat official data.\n",
    "Other sources can be used, just updating the numbers in the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a823322",
   "metadata": {},
   "outputs": [],
   "source": [
    "exchange_euro_nok = pd.read_csv(\n",
    "    data_path / \"exchange_rates.csv\", sep=\"\\t\", index_col=0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abed9639",
   "metadata": {},
   "source": [
    "The Norwegian IO data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5683cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "nor_io_dom_files = {\n",
    "    int(file.stem[-4:]): file for file in data_path.glob(\"iot_1850_*.xlsx\")\n",
    "}\n",
    "nor_io_imp_files = {\n",
    "    int(file.stem[-4:]): file for file in data_path.glob(\"iot_1950_*.xlsx\")\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c671caf8",
   "metadata": {},
   "source": [
    "Then we specify the file with the extracted EXIOBASE data.\n",
    "This needs to be prepared for each new EXIOBASE version.\n",
    "The utility script ./exio_prepper.py can be used for this.\n",
    "For version 3.8.2, the file is already provided in the data folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8413f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "exio_extract_file: Path = data_path / \"exio_no_bp_raw.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20f52ba",
   "metadata": {},
   "source": [
    "Last we define the emissions of interest.\n",
    "These needs to be the name of the sheets in the emission file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8331ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_in_nor_data = [\"CO2\", \"Biomass CO2\", \"N2O\", \"SF6_NF3\", \"CH4\", \"HFC\", \"PFC\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625aed3d",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ce4eef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Here we defined a function used throught the script to check the data\n",
    "and bring it to a consistent format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad764c8d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def df_checks(df, name, report_nan=False, report_inf=True, report_str=True):\n",
    "    \"\"\"Checks all non-float columns, report and fix\n",
    "\n",
    "    Checks and fixes:\n",
    "\n",
    "        - dtypes (cast to float if possible)\n",
    "        - NaN (set to 0.0)\n",
    "        - Inf (set to 0.0)\n",
    "        - strings (set to 0.0)\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame or pd.Series\n",
    "        The dataframe to check\n",
    "    name : str\n",
    "        The name of the dataframe, used for reporting.\n",
    "    report_nan : bool, optional\n",
    "        Whether to report NaN values, by default False\n",
    "    report_inf : bool, optional\n",
    "        Whether to report Inf values, by default True\n",
    "    report_str : bool, optional\n",
    "        Whether to report string values, by default True\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame or pd.Series, depending on input\n",
    "    \"\"\"\n",
    "    if type(df) == pd.Series:\n",
    "        df = df.to_frame()\n",
    "        return_back_to_series = True\n",
    "    else:\n",
    "        return_back_to_series = False\n",
    "\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype in (float, np.float64):\n",
    "            pass\n",
    "        elif df[col].dtype == int:\n",
    "            # int can be just cast to float\n",
    "            df[col] = df[col].astype(float)\n",
    "        elif df[col].dtype == object:\n",
    "            # check for strings in column and report these\n",
    "            row_str = df[col].apply(lambda x: isinstance(x, str))\n",
    "            if row_str.any():\n",
    "                for row in df[row_str].index:\n",
    "                    if report_str:\n",
    "                        print(\n",
    "                            f\"WARNING: Table {name} - Column {col} contains string {df.loc[row, col]}. Set to 0.0.\"\n",
    "                        )\n",
    "                    df.loc[row, col] = 0.0\n",
    "            df[col] = df[col].astype(float)\n",
    "        else:\n",
    "            print(\n",
    "                f\"WARNING: {name} - {col} has unhandled dtype {df[col].dtype}.\"\n",
    "            )\n",
    "\n",
    "        # check for NaN and Inf\n",
    "        if df[col].isna().any():\n",
    "            if report_nan:\n",
    "                print(\n",
    "                    f\"WARNING: Table {name} - Column {col} contains NaN values. Set to 0.0.\"\n",
    "                )\n",
    "            df[col].fillna(0.0, inplace=True)\n",
    "        if np.isinf(df[col].values).any():\n",
    "            if report_inf:\n",
    "                print(\n",
    "                    f\"WARNING: Table {name} - Column {col} contains Inf values. Set to 0.0.\"\n",
    "                )\n",
    "            df[col][np.isinf(df[col].values)] = 0.0\n",
    "\n",
    "    if return_back_to_series:\n",
    "        df = df.squeeze()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b147ef40",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Read the official Norwegian emission data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6094667f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Function for reading the official Norwegian emission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8211d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nor_emissions(emission_type, years, emission_file, industry_codes):\n",
    "    \"\"\"Read the official Norwegian emission data for a given emission type.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    emission_type : str\n",
    "        The emission type to read.\n",
    "    years : list[int]\n",
    "        The years to read.\n",
    "    emission_file : Path\n",
    "        The path to the emission file.\n",
    "    industry_codes : list[str]\n",
    "        The industry codes to read.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    emis_nor : pd.DataFrame\n",
    "        The emission data for the given emission type.\n",
    "    emis_nor_hhld : pd.DataFrame\n",
    "        The household emission data for the given emission type.\n",
    "    \"\"\"\n",
    "\n",
    "    raw_emission_data = pd.read_excel(\n",
    "        emission_file, sheet_name=emission_type, header=3\n",
    "    )\n",
    "\n",
    "    emis_nor = pd.DataFrame(index=industry_codes, columns=years)\n",
    "\n",
    "    year_columns = [year for year in years if year in raw_emission_data.columns]\n",
    "\n",
    "    slice_hhld = slice(86, 90)\n",
    "    name_index_hhld_total = (86, 1)\n",
    "    name_index_hhld_breakdown = (slice(87, 90), 3)\n",
    "\n",
    "    for icode in industry_codes:\n",
    "        indx = raw_emission_data[\n",
    "            raw_emission_data[\"Air emissions by industry\"] == icode\n",
    "        ].index\n",
    "        if not indx.empty:\n",
    "            indx = indx[0]\n",
    "            data = raw_emission_data.loc[indx, year_columns].values\n",
    "            emis_nor.loc[icode, year_columns] = data\n",
    "\n",
    "    hhld_total_name = (\n",
    "        raw_emission_data.iloc[*name_index_hhld_total].split(\"\\n\")[1].strip()\n",
    "    )\n",
    "    hhld_breakdown_name = (\n",
    "        raw_emission_data.iloc[*name_index_hhld_breakdown]\n",
    "        .str.strip(\"-\")\n",
    "        .str.strip()\n",
    "    )\n",
    "    hhld_names = [hhld_total_name] + hhld_breakdown_name.tolist()\n",
    "\n",
    "    emis_nor_hhld = raw_emission_data.loc[:, year_columns].iloc[slice_hhld, :]\n",
    "    emis_nor_hhld.index = hhld_names\n",
    "\n",
    "    emis_nor = df_checks(\n",
    "        emis_nor,\n",
    "        \"Norwegian industry emissions\",\n",
    "        report_nan=True,\n",
    "        report_inf=True,\n",
    "        report_str=True,\n",
    "    )\n",
    "    emis_nor_hhld = df_checks(\n",
    "        emis_nor_hhld,\n",
    "        \"Norwegian household emissions\",\n",
    "        report_nan=True,\n",
    "        report_inf=True,\n",
    "        report_str=True,\n",
    "    )\n",
    "\n",
    "    # Put data from row c19 into row c21 - to be consistent with IO data.\n",
    "    emis_nor.loc[\"C21\"] = emis_nor.loc[\"C19\"]\n",
    "    emis_nor.loc[\"C19\"] = 0\n",
    "\n",
    "    emis_nor_hhld.index.name = \"emission_type\"\n",
    "    emis_nor_hhld.columns.name = \"activity\"\n",
    "    emis_nor.index.name = \"emission_type\"\n",
    "\n",
    "    return emis_nor, emis_nor_hhld"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d030132e",
   "metadata": {},
   "source": [
    "Next, we read the emission data into two\n",
    "dictionaries (one for industry and one for households) and then\n",
    "convert these to yearly emission data with emission type as rows\n",
    "and industry as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ced6ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_type_dict = {}\n",
    "emis_type_dict_hhld = {}\n",
    "\n",
    "for emission in emis_in_nor_data:\n",
    "    print(f\"PROCESSING emission data for {emission}.\")\n",
    "    emis_type_dict[emission], emis_type_dict_hhld[emission] = get_nor_emissions(\n",
    "        emission_type=emission,\n",
    "        years=years,\n",
    "        emission_file=nor_emission_file,\n",
    "        industry_codes=sector_list.IndustryCode.dropna(),\n",
    "    )\n",
    "\n",
    "emis_col_ind = {}\n",
    "emis_col_hhld = {}\n",
    "\n",
    "for year in years:\n",
    "    emis_col_ind[year] = pd.DataFrame(\n",
    "        data=[\n",
    "            emis_type_dict[emission][year].T for emission in emis_in_nor_data\n",
    "        ],\n",
    "        index=emis_in_nor_data,\n",
    "    )\n",
    "    emis_col_hhld[year] = pd.DataFrame(\n",
    "        data=[\n",
    "            emis_type_dict_hhld[emission][year].to_dict()\n",
    "            for emission in emis_in_nor_data\n",
    "        ],\n",
    "        index=emis_in_nor_data,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d5be4e",
   "metadata": {},
   "source": [
    "Then we characterize the Norwegian emission data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6cb8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_nor_ind = {}\n",
    "emis_nor_hhld = {}\n",
    "for year in years:\n",
    "    ghg_name = charact_ghg.impact.unique()\n",
    "    if len(ghg_name) > 1:\n",
    "        raise NotImplementedError(\n",
    "            \"More than one characerized impact in characterization factors.\"\n",
    "        )\n",
    "    else:\n",
    "        ghg_name = ghg_name[0]\n",
    "\n",
    "    # First we convert to GHGeq based on characterization factors.\n",
    "    # These are not stored currently, but we could save them if needed.\n",
    "    emis_ind_co2eq = emis_col_ind[year].multiply(charact_ghg.factor, axis=0)\n",
    "    emis_hhld_co2eq = emis_col_hhld[year].multiply(charact_ghg.factor, axis=0)\n",
    "\n",
    "    # Then we sum got GHG totals\n",
    "    emis_ind_co2eq.loc[ghg_name, :] = emis_ind_co2eq.sum(axis=0).fillna(0)\n",
    "    emis_hhld_co2eq.loc[ghg_name, :] = emis_hhld_co2eq.sum(axis=0).fillna(0)\n",
    "\n",
    "    # And add them to the native (non CO2eq) format data\n",
    "    emis_nor_ind[year] = emis_col_ind[year].copy()\n",
    "    emis_nor_hhld[year] = emis_col_hhld[year].copy()\n",
    "    emis_nor_ind[year].loc[ghg_name, :] = emis_ind_co2eq.loc[ghg_name, :]\n",
    "    emis_nor_hhld[year].loc[ghg_name, :] = emis_hhld_co2eq.loc[ghg_name, :]\n",
    "    emis_nor_hhld[year].index.names = [\"emission\"]\n",
    "    emis_nor_hhld[year].columns.names = [\"hhld_component\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97834e06",
   "metadata": {},
   "source": [
    "Then we rename columns to match the Norwegian economic sector names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894001ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_dict = (\n",
    "    sector_list.loc[:, [\"IndustryCode\", \"Product\"]]\n",
    "    .dropna(how=\"any\")\n",
    "    .set_index(\"IndustryCode\")\n",
    "    .squeeze()\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "for year in years:\n",
    "    emis_nor_ind[year] = emis_nor_ind[year].rename(columns=rename_dict)\n",
    "    # strip whitespace since \"Coke and refined...\" sector has whitespace\n",
    "    # in the official data\n",
    "    emis_nor_ind[year].columns = emis_nor_ind[year].columns.str.strip()\n",
    "    emis_nor_ind[year].index.name = \"emission\"\n",
    "    emis_nor_ind[year].columns.name = \"sector\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225bd7bd",
   "metadata": {},
   "source": [
    "Next we make a summary dataframe holding the units for each emission type.\n",
    "We also order that and use this order during saving the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ff62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emis_nor_unit = charact_ghg.loc[:, \"stressor_unit\"]\n",
    "emis_nor_unit.name = \"unit\"\n",
    "emis_nor_unit.loc[ghg_name] = charact_ghg.loc[:, \"impact_unit\"].unique()[0]\n",
    "emis_nor_unit.index.name = \"emission\"\n",
    "# order the rows by impact and then the order of stressor in charact_ghg\n",
    "_sort_emis = (\n",
    "    charact_ghg.impact.unique().tolist() + charact_ghg.index.unique().tolist()\n",
    ")\n",
    "emis_nor_unit = emis_nor_unit.loc[_sort_emis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33923b",
   "metadata": {},
   "source": [
    "## IO reading functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44e028",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Function for reading one year from the Norwegian IO tables.\n",
    "The official IO tables have a quite complex structure, so\n",
    "the function below is quite complex as well. There are\n",
    "several \"magic strings and numbers\" which refer to specific\n",
    "cells in the excel files. These will need to be\n",
    "updated if the structure of the excel files changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f46a7bb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def get_nor_IO(file_dom, file_imp, emis_nor_df):\n",
    "    \"\"\"Read the official Norwegian IO tables and calculate L, x, SL.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    file_dom : Path\n",
    "        The path to the domestic IO table.\n",
    "\n",
    "    file_imp : Path\n",
    "        The path to the import IO table.\n",
    "\n",
    "    emis_nor_df : pd.DataFrame\n",
    "        The emission data for the specific year.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    product_names : pd.DataFrame\n",
    "\n",
    "    nor_io : dict\n",
    "        IO tables in a dictionary.\n",
    "    \"\"\"\n",
    "    # Sheet definitions\n",
    "    import_sheet = \"1950\"\n",
    "    domestic_sheet = \"1850\"\n",
    "\n",
    "    # Cell definitions\n",
    "    cells_product_names = \"D29:D93\"\n",
    "    cells_Ydom_name = \"BS27:CG27\"\n",
    "    cells_Yimp_name = \"BS27:CG27\"\n",
    "    # The final use string is one row above the others ¯\\_(ツ)_/¯\n",
    "    cell_Ydom_final_use_name = \"CG26\"\n",
    "    cell_Yimp_final_use_name = \"CG26\"\n",
    "\n",
    "    cells_Zdom = \"E29:BQ93\"\n",
    "    cells_Ydom = \"BS29:CG93\"\n",
    "\n",
    "    cells_Zimp = \"E29:BQ93\"\n",
    "    cells_Yimp = \"BS29:CG93\"\n",
    "\n",
    "    cells_x_tot = \"CH29:CH93\"\n",
    "\n",
    "    def cell_reader(wb_sheet, cell_range):\n",
    "        \"\"\"Helper function for reading cell ranges\"\"\"\n",
    "        data = []\n",
    "        for row in wb_sheet[cell_range]:\n",
    "            data.append([cell.value for cell in row])\n",
    "        df = pd.DataFrame(data)\n",
    "        return df\n",
    "\n",
    "    wb_dom = opx.load_workbook(file_dom, data_only=True)\n",
    "    assert (\n",
    "        domestic_sheet in wb_dom.sheetnames\n",
    "    ), f\"Sheet {domestic_sheet} not found in file_dom, structure changed\"\n",
    "\n",
    "    prod_names = (\n",
    "        cell_reader(wb_dom[domestic_sheet], cells_product_names)\n",
    "        .iloc[:, 0]\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    Zdom = cell_reader(wb_dom[domestic_sheet], cells_Zdom)\n",
    "    Zdom.columns = prod_names\n",
    "    Zdom.columns.name = \"sector\"\n",
    "    Zdom.index = prod_names\n",
    "    Zdom.index.name = \"sector\"\n",
    "    Zdom = df_checks(Zdom, \"Zdom\")\n",
    "\n",
    "    Ydom = cell_reader(wb_dom[domestic_sheet], cells_Ydom)\n",
    "    Ydom_names = cell_reader(wb_dom[domestic_sheet], cells_Ydom_name).iloc[0, :]\n",
    "    Ydom_names.iloc[-1] = wb_dom[domestic_sheet][cell_Ydom_final_use_name].value\n",
    "    # strip white space and carriage return\n",
    "    Ydom_names = [name.strip() for name in Ydom_names]\n",
    "    Ydom.columns = Ydom_names\n",
    "    Ydom.columns.name = \"category\"\n",
    "    Ydom.index = prod_names\n",
    "    Ydom.index.name = \"sector\"\n",
    "    Ydom = df_checks(Ydom, \"Ydom\")\n",
    "\n",
    "    xdom = cell_reader(wb_dom[domestic_sheet], cells_x_tot).fillna(0).iloc[:, 0]\n",
    "    xdom.index = prod_names\n",
    "    xdom.index.name = \"sector\"\n",
    "    xdom = df_checks(xdom, \"xdom\")\n",
    "\n",
    "    wb_dom.close()\n",
    "\n",
    "    wb_imp = opx.load_workbook(file_imp, data_only=True)\n",
    "    assert (\n",
    "        import_sheet in wb_imp.sheetnames\n",
    "    ), f\"Sheet {import_sheet} not found in file_dom, structure changed\"\n",
    "\n",
    "    prod_names_imp = (\n",
    "        cell_reader(wb_imp[import_sheet], cells_product_names)\n",
    "        .iloc[:, 0]\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    assert prod_names_imp.equals(\n",
    "        prod_names\n",
    "    ), \"Product names in import and domestic IO tables do not match\"\n",
    "\n",
    "    Zimp = cell_reader(wb_imp[import_sheet], cells_Zimp).fillna(0)\n",
    "    Zimp.columns = prod_names\n",
    "    Zimp.columns.name = \"sector\"\n",
    "    Zimp.index = prod_names\n",
    "    Zimp.index.name = \"sector\"\n",
    "    Zimp = df_checks(Zimp, \"Zimp\")\n",
    "\n",
    "    Yimp_raw = cell_reader(wb_imp[import_sheet], cells_Yimp).fillna(0)\n",
    "    Yimp_names = cell_reader(wb_imp[import_sheet], cells_Yimp_name).iloc[0, :]\n",
    "    Yimp_names.iloc[-1] = wb_imp[import_sheet][cell_Yimp_final_use_name].value\n",
    "    # strip white space and carriage return\n",
    "    Yimp_names = [name.strip() for name in Yimp_names]\n",
    "    Yimp_raw.columns = Yimp_names\n",
    "    Yimp_raw.columns.name = \"category\"\n",
    "    Yimp_raw.index = prod_names\n",
    "    Yimp_raw.index.name = \"sector\"\n",
    "    Yimp_raw = df_checks(Yimp_raw, \"Yimp\")\n",
    "\n",
    "    ximp = cell_reader(wb_imp[import_sheet], cells_x_tot).fillna(0).iloc[:, 0]\n",
    "    ximp.index = prod_names\n",
    "    ximp.index.name = \"sector\"\n",
    "    ximp = df_checks(ximp, \"ximp\")\n",
    "\n",
    "    wb_imp.close()\n",
    "\n",
    "    nor_io_tot = {}\n",
    "\n",
    "    nor_io_tot[\"Zdom\"] = Zdom\n",
    "\n",
    "    nor_io_tot[\"Ydom_exp\"] = Ydom.loc[:, \"Exports fob (2)\"]\n",
    "\n",
    "    # Full domestic final demand matrix without aggregates\n",
    "    # The list below shows all column headers, the ones not used\n",
    "    # are commented out - just for manual control.\n",
    "    nor_io_tot[\"Ydom_br\"] = Ydom.loc[\n",
    "        :,\n",
    "        [\n",
    "            \"Final consumption expenditure by households\",\n",
    "            \"Final consumption expenditure by non-profit organisations serving households (NPISH)\",\n",
    "            \"Final consumption expenditure by government\",\n",
    "            # 'Final consumption expenditure',\n",
    "            \"Gross fixed capital formation\",\n",
    "            # 'Changes in valuables (1)',\n",
    "            \"Changes in inventories\",\n",
    "            #'Changes in inventories and valuables',\n",
    "            #'Gross capital formation',\n",
    "            #'Exports intra EU fob (1)',\n",
    "            #'Exports fob to members of the euro area (1)',\n",
    "            #'Exports fob to non-members of the euro area (1)',\n",
    "            #'Exports extra EU fob (1)',\n",
    "            \"Exports fob (2)\",\n",
    "            #'Final uses']   # Excluding final uses as these are recalculated\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Domestic final demand (excluding exports)\n",
    "    # nor_io_tot[\"Ydom\"] = Ydom.loc[:, \"Final uses\"] - nor_io_tot[\"Ydom_exp\"]\n",
    "\n",
    "    # Setting negatives to zero, important for GCF and changes in valueables\n",
    "    nor_io_tot[\"Ydom_br\"][nor_io_tot[\"Ydom_br\"] < 0] = 0\n",
    "\n",
    "    nor_io_tot[\"Ydom\"] = (\n",
    "        nor_io_tot[\"Ydom_br\"].sum(axis=1) - nor_io_tot[\"Ydom_exp\"]\n",
    "    )\n",
    "\n",
    "    # Import flow matrix\n",
    "    nor_io_tot[\"Zimp\"] = Zimp\n",
    "\n",
    "    # Import-exports (re-exports)\n",
    "    nor_io_tot[\"Yimp_exp\"] = Yimp_raw.loc[:, \"Exports fob (2)\"]\n",
    "\n",
    "    nor_io_tot[\"Yimp_raw\"] = Yimp_raw\n",
    "\n",
    "    # Full import final demand matrix without aggregates\n",
    "    # The list below shows all column headers, the ones not used\n",
    "    # are commented out - just for manual control.\n",
    "    nor_io_tot[\"Yimp_br\"] = Yimp_raw.loc[\n",
    "        :,\n",
    "        [\n",
    "            \"Final consumption expenditure by households\",\n",
    "            \"Final consumption expenditure by non-profit organisations serving households (NPISH)\",\n",
    "            \"Final consumption expenditure by government\",\n",
    "            # 'Final consumption expenditure',\n",
    "            \"Gross fixed capital formation\",\n",
    "            # 'Changes in valuables 1)',\n",
    "            \"Changes in inventories\",\n",
    "            # 'Changes in inventories and valuables',\n",
    "            # 'Gross capital formation',\n",
    "            # 'Exports intra EU fob (1)',\n",
    "            # 'Exports fob to members of the euro area (1)',\n",
    "            # 'Exports fob to non-members of the euro area (1)',\n",
    "            # 'Exports extra EU fob (1)',\n",
    "            \"Exports fob (2)\",\n",
    "            # 'Final uses at basic prices',\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    # Setting negatives to zero, important for GCF and changes in valueables\n",
    "    nor_io_tot[\"Yimp_br\"][nor_io_tot[\"Yimp_br\"] < 0] = 0\n",
    "\n",
    "    nor_io_tot[\"Yimp\"] = (\n",
    "        nor_io_tot[\"Yimp_br\"].sum(axis=1) - nor_io_tot[\"Yimp_exp\"]\n",
    "    )\n",
    "\n",
    "    # Recalculating x considering removed negatives\n",
    "    nor_io_tot[\"x\"] = (\n",
    "        nor_io_tot[\"Zdom\"].sum(axis=1)\n",
    "        + nor_io_tot[\"Ydom\"]\n",
    "        + nor_io_tot[\"Ydom_exp\"]\n",
    "    )\n",
    "\n",
    "    nor_io_tot[\"Adom\"] = pymrio.calc_A(nor_io_tot[\"Zdom\"], nor_io_tot[\"x\"])\n",
    "    nor_io_tot[\"Aimp\"] = pymrio.calc_A(nor_io_tot[\"Zimp\"], nor_io_tot[\"x\"])\n",
    "    nor_io_tot[\"Ldom\"] = pymrio.calc_L(nor_io_tot[\"Adom\"])\n",
    "\n",
    "    nor_io_tot[\"Sdom\"] = pymrio.calc_S(emis_nor_df, nor_io_tot[\"x\"])\n",
    "\n",
    "    nor_io_tot[\"SLdom\"] = pymrio.calc_M(\n",
    "        S=nor_io_tot[\"Sdom\"], L=nor_io_tot[\"Ldom\"]\n",
    "    )\n",
    "    nor_io_tot[\"SLdom\"]\n",
    "\n",
    "    return nor_io_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0a80e2",
   "metadata": {},
   "source": [
    "## Main coupling and calculation step"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4ccc502",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "First we define several function for coupling and calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d2415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snac_coupling(Aimp, Ldom, Qimp_mrio):\n",
    "    \"\"\"Coupling of MRIO based multipliers with domestic IO tables\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "\n",
    "    Aimp: pd.DataFrame\n",
    "        Import A matrix\n",
    "    Ldom: pd.DataFrame\n",
    "        Domestic L matrix\n",
    "    Qimp_mrio: pd.DataFrame\n",
    "        Import multipliers from MRIO\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict with\n",
    "        QAimp: pd.DataFrame\n",
    "            QAimp matrix, TODO: explain\n",
    "        QAimpLdom: pd.DataFrame\n",
    "            QAimpLdom matrix\n",
    "    \"\"\"\n",
    "    QAimp = Qimp_mrio @ Aimp\n",
    "    QAimpLdom = QAimp @ Ldom\n",
    "\n",
    "    return dict(QAimp=QAimp, QAimpLdom=QAimpLdom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a129df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def snac_findem_breakdown(SLdom_diag, QAimp_Ldom_diag, Q_imp_exio_diag, Ydom_br, Yimp_br):\n",
    "    \"\"\"Breakdown of the total fp into final demand categories\n",
    "\n",
    "    Components: domestic_footprint, import_footprint, total\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    SLdom_diag: pd.DataFrame\n",
    "        Diagonalized SLdom matrix for one stressor/gas/impact\n",
    "    QAimp_Ldom_diag: pd.DataFrame\n",
    "        Diagonalized QAimpLdom matrix for one stressor/gas/impact\n",
    "    Q_imp_exio_diag: pd.DataFrame\n",
    "        Diagonalized Q_imp_exio matrix for one stressor/gas/impact\n",
    "    Ydom_br: pd.DataFrame\n",
    "        domestic_footprint final demand matrix without aggregates\n",
    "    Yimp_br: pd.DataFrame\n",
    "        Import final demand matrix without aggregates\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    pd.Series\n",
    "\n",
    "        index: pd.MultiIndex\n",
    "\n",
    "            level 0: component\n",
    "                domestic_footprint, import_footprint, total_footprint\n",
    "            level 1: sector\n",
    "                sector names\n",
    "            level 2: category\n",
    "                final demand category\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    dom_breakdown = SLdom_diag @ Ydom_br\n",
    "    imp_breakdown = QAimp_Ldom_diag @ Ydom_br + Q_imp_exio_diag @ Yimp_br\n",
    "    tot_footprint_breakdown = dom_breakdown + imp_breakdown\n",
    "\n",
    "    index_names = [\"component\"] + list(dom_breakdown.index.names)\n",
    "\n",
    "    ds = pd.concat(\n",
    "        [dom_breakdown, imp_breakdown, tot_footprint_breakdown],\n",
    "        keys=[\"domestic_footprint\", \"import_footprint\", \"total_footprint\"],\n",
    "        names=index_names,\n",
    "        axis=0,\n",
    "    ).stack()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4aa5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_snac_sector_accounts(\n",
    "    SLdom,\n",
    "    QAimp_Ldom,\n",
    "    Ydom_diag,\n",
    "    Qimp_mrio,\n",
    "    Yimp_diag,\n",
    "    Ydom_exp_diag,\n",
    "    Yimp_exp_diag,\n",
    "    total_imports,\n",
    "    pba,\n",
    "):\n",
    "    \"\"\"Calculate and merge sector accounts\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    SLdom: pd.DataFrame\n",
    "\n",
    "    QAimp_Ldom: pd.DataFrame\n",
    "\n",
    "    Qimp_mrio: pd.DataFrame\n",
    "\n",
    "    Ydom_diag: pd.DataFrame\n",
    "\n",
    "    Yimp_diag: pd.DataFrame\n",
    "\n",
    "    Ydom_exp_diag: pd.DataFrame\n",
    "\n",
    "    Yimp_exp_diag: pd.DataFrame\n",
    "\n",
    "    total_imports: pd.DataFrame\n",
    "\n",
    "    pba: pd.DataFrame\n",
    "\n",
    "    Returns\n",
    "    --------\n",
    "    pd.Series\n",
    "\n",
    "        index: pd.MultiIndex\n",
    "\n",
    "            level 0: component\n",
    "                domestic_footprint, import_footprint, total_footprint\n",
    "            level 1: sector\n",
    "                sector names\n",
    "            level 2: category\n",
    "                final demand category\n",
    "    \"\"\"\n",
    "    # TODO: level 2 above wrong?\n",
    "\n",
    "    total_footprint = (\n",
    "        SLdom @ Ydom_diag + QAimp_Ldom @ Ydom_diag + Qimp_mrio @ Yimp_diag\n",
    "    )\n",
    "\n",
    "    domestic_footprint = SLdom * Ydom_diag.sum(axis=1)\n",
    "    import_footprint = QAimp_Ldom @ Ydom_diag + Qimp_mrio @ Yimp_diag\n",
    "    export_footprint = (\n",
    "        SLdom @ Ydom_exp_diag\n",
    "        + QAimp_Ldom @ Ydom_exp_diag\n",
    "        + Qimp_mrio @ Yimp_exp_diag\n",
    "    )\n",
    "\n",
    "    footprint_gross_imports = Qimp_mrio * total_imports\n",
    "    footprint_gross_imports.columns.name = \"sector\"\n",
    "\n",
    "    index_names = [\"component\", \"emission\"]\n",
    "\n",
    "    ds = pd.concat(\n",
    "        [\n",
    "            total_footprint,\n",
    "            domestic_footprint,\n",
    "            import_footprint,\n",
    "            export_footprint,\n",
    "            footprint_gross_imports,\n",
    "            pba,\n",
    "        ],\n",
    "        keys=[\n",
    "            \"total_footprint\",\n",
    "            \"domestic_footprint\",\n",
    "            \"import_footprint\",\n",
    "            \"footprint_gross_exports\",\n",
    "            \"footprint_gross_imports\",\n",
    "            \"production_account\",\n",
    "        ],\n",
    "        names=index_names,\n",
    "        axis=0,\n",
    "    ).stack()\n",
    "\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66e0638",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def exio_nor_converter(to_conv, sector_matching, new_col_names=\"name\"):\n",
    "    \"\"\"Converts classification from EXIOBASE to Norwegian sectors\n",
    "\n",
    "    This checks if the columns of to_conv matches on of\n",
    "    the index of sector_matching.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    to_conv: pd.DataFrame\n",
    "        Accounts to convert to Norwegian sectors.\n",
    "        EXIOBASE classification in the columns.\n",
    "\n",
    "    sector_matching: pd.DataFrame\n",
    "        Matching between EXIOBASE and Norwegian sectors.\n",
    "        Read from sector matching file.\n",
    "\n",
    "    new_col_names: str\n",
    "        Can be 'name', 'code' or 'nr'.\n",
    "        Taken from sector_matching columns and used as the new\n",
    "        sector names.\n",
    "\n",
    "    Returns\n",
    "\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Accounts converted to Norwegian sectors\n",
    "    \"\"\"\n",
    "    # As we do an concordance based conversion, we check for the correct order\n",
    "    # for at least on of the index levels of sector_matching\n",
    "    for ind_number in range(0, 100):\n",
    "        try:\n",
    "            sec_match_order = sector_matching.index.get_level_values(ind_number)\n",
    "            if sec_match_order.equals(to_conv.columns):\n",
    "                break\n",
    "        except IndexError:\n",
    "            raise ValueError(\"Could not find matching index value do not match\")\n",
    "\n",
    "    ret = to_conv @ sector_matching.values\n",
    "    ret.columns = sector_matching.columns.get_level_values(\n",
    "        new_col_names\n",
    "    ).str.strip()\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a132ae4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chem_sector_agg(df):\n",
    "    \"\"\" Aggregates the three chemical sectors to one sector.\n",
    "\n",
    "    Due to confidentiality of data for the three sectors below in both Norwegian emissions and economic data,\n",
    "    the data is aggregated to one sector, to be consistent with the Norwegian IO data\n",
    "    NOTE: the original sector name for \"Coke ...\" has a whitespace at the end.\n",
    "    We removed that whitespace during reading the files.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pd.DataFrame\n",
    "        Accounts in Norwegian classification in the columns.\n",
    "        Rows can be anything.\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Accounts with the three chemical sectors aggregated to one sector.\n",
    "        Rest of the accounts/rows are unchanged.\n",
    "\n",
    "    \"\"\"\n",
    "    chem_sectors_to_sum = [\n",
    "        \"Coke and refined petroleum products\",\n",
    "        \"Chemicals and chemical products\",\n",
    "        \"Basic pharmaceutical products and pharmaceutical preparations\",\n",
    "    ]\n",
    "\n",
    "    chem_sector_agg = (\n",
    "        \"Basic pharmaceutical products and pharmaceutical preparations\"\n",
    "    )\n",
    "\n",
    "    chem_sum = df.loc[:, chem_sectors_to_sum].sum(axis=1)\n",
    "    df.loc[:, chem_sectors_to_sum] = 0\n",
    "    df.loc[:, chem_sector_agg] = chem_sum\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c1ac1",
   "metadata": {},
   "source": [
    "## Main calculation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2666a1",
   "metadata": {},
   "source": [
    "Establish some dictionaries for storing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5326c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_dom_io = {}\n",
    "col_Q_imp_exio = {}\n",
    "col_snac = {}\n",
    "col_findem_breakdown = {}\n",
    "col_sector_accounts = {}\n",
    "col_grand_totals = {}\n",
    "col_sources = {}\n",
    "col_sources_tot = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b460e6",
   "metadata": {},
   "source": [
    "Run the calculation for each year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9de942",
   "metadata": {
    "title": "MAIN LOOP FOR RESULT CALCULATION"
   },
   "outputs": [],
   "source": [
    "for year in years:\n",
    "    print(f\"PROCESSING YEAR: {year}\")\n",
    "\n",
    "    dom_io = get_nor_IO(\n",
    "        file_dom=nor_io_dom_files[year],\n",
    "        file_imp=nor_io_imp_files[year],\n",
    "        emis_nor_df=emis_nor_ind[year],\n",
    "    )\n",
    "\n",
    "    imp_exio_extract = pd.read_excel(\n",
    "        exio_extract_file, sheet_name=f\"imp_{year}\", index_col=0, header=[0, 1]\n",
    "    ).T.reset_index(\"unit\")\n",
    "    imp_exio_extract_unit = imp_exio_extract.loc[:, \"unit\"].squeeze().copy()\n",
    "    imp_exio_extract_unit.index.name = \"account\"\n",
    "    imp_exio_extract_unit.name = \"unit\"\n",
    "    imp_exio_extract = imp_exio_extract.drop(\"unit\", axis=1)\n",
    "\n",
    "    imp_nor = exio_nor_converter(\n",
    "        imp_exio_extract, sector_matching, new_col_names=\"name\"\n",
    "    )\n",
    "    imp_nor.loc[\"imports\"] = (\n",
    "        imp_nor.loc[\"imports\"] * exchange_euro_nok.loc[year, \"euro_to_nok\"]\n",
    "    )\n",
    "    imp_nor_unit = imp_exio_extract_unit.copy()\n",
    "    imp_nor_unit.loc[\"imports\"] = \"M.NOK\"\n",
    "\n",
    "    # Fix emission mismatch\n",
    "    # Due to confidentiality of data for the three sectors below in both Norwegian emissions and economic data,\n",
    "    # the data is aggregated to one sector, to be consistent with the Norwegian IO data\n",
    "    imp_nor = chem_sector_agg(imp_nor)\n",
    "\n",
    "    # We have Q_ something as abbreviation for the (full-supply-chain) multipliers\n",
    "    # throughout the script. Note, that these are called .M in the pymrio package.\n",
    "    Q_imp_exio = (\n",
    "        imp_nor.div(imp_nor.loc[\"imports\", :], axis=1)\n",
    "        .fillna(0)\n",
    "        .drop(\"imports\", axis=0)\n",
    "    )\n",
    "    Q_imp_exio_unit = imp_nor_unit.drop(\"imports\", axis=0)\n",
    "    Q_imp_exio_unit = Q_imp_exio_unit + \"/M.NOK\"\n",
    "\n",
    "    snac = snac_coupling(\n",
    "        Aimp=dom_io[\"Aimp\"], Ldom=dom_io[\"Ldom\"], Qimp_mrio=Q_imp_exio\n",
    "    )\n",
    "\n",
    "    findem_breakdown_gather = dict()\n",
    "    for emission in emis_nor_unit.index:\n",
    "        SLdom_diag = pd.DataFrame(\n",
    "            np.diag(dom_io[\"SLdom\"].loc[emission, :]),\n",
    "            index=dom_io[\"SLdom\"].columns,\n",
    "            columns=dom_io[\"SLdom\"].columns,\n",
    "        )\n",
    "\n",
    "        QAimp_Ldom_diag = pd.DataFrame(\n",
    "            np.diag(snac[\"QAimpLdom\"].loc[emission, :]),\n",
    "            index=snac[\"QAimpLdom\"].columns,\n",
    "            columns=snac[\"QAimpLdom\"].columns,\n",
    "        )\n",
    "        Q_imp_exio_diag = pd.DataFrame(\n",
    "            np.diag(Q_imp_exio.loc[emission, :]),\n",
    "            index=Q_imp_exio.columns,\n",
    "            columns=Q_imp_exio.columns,\n",
    "        )\n",
    "        findem_breakdown_gather[emission] = snac_findem_breakdown(\n",
    "            SLdom_diag=SLdom_diag,\n",
    "            QAimp_Ldom_diag=QAimp_Ldom_diag,\n",
    "            Q_imp_exio_diag=Q_imp_exio_diag,\n",
    "            Ydom_br=dom_io[\"Ydom_br\"],\n",
    "            Yimp_br=dom_io[\"Yimp_br\"],\n",
    "        )\n",
    "\n",
    "    _df = pd.concat(findem_breakdown_gather, axis=1)\n",
    "    _df.columns.names = [\"emission\"]\n",
    "\n",
    "    fp_findem_breakdown = _df.stack()\n",
    "    del _df\n",
    "\n",
    "    # footprint of imports (goods traded at the border)\n",
    "    total_imports = dom_io[\"Zimp\"].sum(axis=1) + dom_io[\"Yimp\"] + dom_io[\"Yimp_exp\"]\n",
    "\n",
    "    sector_accounts = calc_snac_sector_accounts(\n",
    "        SLdom=dom_io[\"SLdom\"],\n",
    "        QAimp_Ldom=snac[\"QAimpLdom\"],\n",
    "        Ydom_diag=pd.DataFrame(\n",
    "            data=np.diag(dom_io[\"Ydom\"]),\n",
    "            index=dom_io[\"Ydom\"].index,\n",
    "            columns=dom_io[\"Ydom\"].index,\n",
    "        ),\n",
    "        Yimp_diag=pd.DataFrame(\n",
    "            data=np.diag(dom_io[\"Yimp\"]),\n",
    "            index=dom_io[\"Yimp\"].index,\n",
    "            columns=dom_io[\"Yimp\"].index,\n",
    "        ),\n",
    "        Ydom_exp_diag=pd.DataFrame(\n",
    "            data=np.diag(dom_io[\"Ydom_exp\"]),\n",
    "            index=dom_io[\"Ydom_exp\"].index,\n",
    "            columns=dom_io[\"Ydom_exp\"].index,\n",
    "        ),\n",
    "        Yimp_exp_diag=pd.DataFrame(\n",
    "            data=np.diag(dom_io[\"Yimp_exp\"]),\n",
    "            index=dom_io[\"Yimp_exp\"].index,\n",
    "            columns=dom_io[\"Yimp_exp\"].index,\n",
    "        ),\n",
    "        Qimp_mrio=Q_imp_exio,\n",
    "        total_imports=total_imports,\n",
    "        pba=emis_nor_ind[year],\n",
    "    )\n",
    "\n",
    "    # fp-src calc: get source of footprint from exiobase\n",
    "    _source = pd.read_excel(\n",
    "        exio_extract_file,\n",
    "        sheet_name=f\"imp_source_{year}\",\n",
    "        index_col=[0, 1],\n",
    "        header=[0],\n",
    "    )\n",
    "    _source.columns.name = \"region\"\n",
    "    _source = _source.stack(\"region\").unstack(\"sector\").reindex(columns = sector_matching.index.get_level_values(\"code\"))\n",
    "\n",
    "    tot_ghg = _source.loc[(\"GHG\")].sum().sum()\n",
    "    tot_ghg\n",
    "    nor_ghg = _source.loc[(\"GHG\", \"NO\")].sum()\n",
    "    nor_ghg\n",
    "    tot_ghg - nor_ghg\n",
    "\n",
    "    # fp-src calc: convert to nor class and do the chem sector adjustment\n",
    "    source_nor_class = exio_nor_converter(\n",
    "        _source, sector_matching, new_col_names=\"name\"\n",
    "    )\n",
    "    source_nor_class.columns.name = \"sector\"\n",
    "    source_nor_class = chem_sector_agg(source_nor_class)\n",
    "\n",
    "    # fp-src: get src scaled import mulitpliers for each region\n",
    "    Q_src_imp = source_nor_class.div(imp_nor.loc[\"imports\", :], axis=1).fillna(0).replace(np.inf, 0)\n",
    "\n",
    "    # fp-src: multiply by imports and dom demand\n",
    "    Q_Aimp_src = Q_src_imp @ dom_io[\"Aimp\"]\n",
    "    Q_Aimp_Ldom_src = Q_Aimp_src @ dom_io[\"Ldom\"]\n",
    "\n",
    "    # fp-src: multiply by final demand and add norwegian domestic demand\n",
    "    src_all = Q_Aimp_Ldom_src * dom_io[\"Ydom\"] + Q_src_imp * dom_io[\"Yimp\"]\n",
    "    src_all.loc[(slice(None), \"NO\"), :] = src_all.loc[(slice(None), \"NO\"), :] + dom_io[\"SLdom\"] * dom_io[\"Ydom\"]\n",
    "    src_all = src_all.stack()\n",
    "    src_all.name = 'value'\n",
    "\n",
    "    src_total = src_all.groupby(level=['region', 'emission']).sum()\n",
    "\n",
    "    # calculate some convenient totals\n",
    "    total_accounts = pd.DataFrame(\n",
    "        sector_accounts.groupby([\"component\", \"emission\"]).sum(),\n",
    "        columns=[\"value\"],\n",
    "    )\n",
    "    total_hhld = pd.DataFrame(emis_nor_hhld[year].loc[:, \"Households, totals\"])\n",
    "    total_hhld.columns = [\"value\"]\n",
    "    tot_fp_with_hhld = (\n",
    "        (total_accounts.loc[\"total_footprint\"] + total_hhld)\n",
    "        .assign(component=\"total_footprint_with_households\")\n",
    "        .set_index(\"component\", append=True)\n",
    "        .reorder_levels([\"component\", \"emission\"])\n",
    "    )\n",
    "    tot_pba_with_hhld = (\n",
    "        (total_accounts.loc[\"production_account\"] + total_hhld)\n",
    "        .assign(component=\"production_account_with_households\")\n",
    "        .set_index(\"component\", append=True)\n",
    "        .reorder_levels([\"component\", \"emission\"])\n",
    "    )\n",
    "    total_hhld = (\n",
    "        total_hhld.assign(component=\"household_totals\")\n",
    "        .set_index(\"component\", append=True)\n",
    "        .reorder_levels([\"component\", \"emission\"])\n",
    "    )\n",
    "    total_accounts = pd.concat(\n",
    "        [total_accounts, tot_fp_with_hhld, tot_pba_with_hhld, total_hhld],\n",
    "        axis=0,\n",
    "    ).squeeze()\n",
    "\n",
    "    col_dom_io[year] = dom_io\n",
    "    col_Q_imp_exio[year] = Q_imp_exio\n",
    "    col_snac[year] = snac\n",
    "    col_findem_breakdown[year] = fp_findem_breakdown\n",
    "    col_sector_accounts[year] = sector_accounts\n",
    "    col_grand_totals[year] = total_accounts\n",
    "    col_sources[year] = src_all\n",
    "    col_sources_tot[year] = src_total"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e91efb8",
   "metadata": {},
   "source": [
    "## Preparing and storing the results\n",
    "For storing the results we stack them together in a long list format.\n",
    "This can easily be converted to any data format, including SQL.\n",
    "In excel, the list can easily be pivoted to a wide format or filtered for specific years.\n",
    "In python, this can be extended to a wide format with .unstack(index_name).\n",
    "We add the unit to the list already here, to make it easier to work with in excel.\n",
    "For subsequent calculations, the unit might be cumbersome\n",
    "and can be dropped with .drop('unit', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5daf8131",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Helper function for stacking, naming and unit assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed95cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stack_and_unit(dd, emis_units, sector_order):\n",
    "    \"\"\"Stacks dataframes for final save and assign units\n",
    "\n",
    "    The results are ordered\n",
    "        - emission: order given in emis_units\n",
    "        - sectors: order given in sector_order\n",
    "        - years: ascending\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dd : dict\n",
    "        With years as keys and data-frames as values\n",
    "    emis_units : pd.Series\n",
    "        Emission units with index of emission names.\n",
    "        The order of the index is used for the order of\n",
    "        the result.\n",
    "    sector_order : list\n",
    "        Orders list of sectors for reordering the results\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        index: Multiindex with year and the remaining ones from the dd values\n",
    "        columns: value, unit\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    merged = pd.concat(dd, axis=0, names=[\"year\"])\n",
    "\n",
    "    while type(merged) is pd.DataFrame:\n",
    "        merged = merged.stack()\n",
    "\n",
    "    merged.name = \"value\"\n",
    "    _ix = merged.index\n",
    "    df = (\n",
    "        merged.reset_index()\n",
    "        .merge(emis_nor_unit, on=\"emission\")\n",
    "        .set_index(_ix.names)\n",
    "    )\n",
    "    df = df.sort_index(level=\"year\", sort_remaining=False).reindex(\n",
    "        emis_units.index.tolist(), level=\"emission\"\n",
    "    )\n",
    "\n",
    "    if \"sector\" in df.index.names:\n",
    "        df = df.reindex(sort_sector, level=\"sector\")\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd031333",
   "metadata": {},
   "source": [
    "Stacking the results into a long list format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0981a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_sector = dom_io[\"x\"].index.tolist()\n",
    "\n",
    "all_sector_accounts = stack_and_unit(\n",
    "    col_sector_accounts, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")\n",
    "all_findem_breakdown = stack_and_unit(\n",
    "    col_findem_breakdown, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")\n",
    "all_hhld = stack_and_unit(\n",
    "    emis_nor_hhld, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")\n",
    "all_sources = stack_and_unit(\n",
    "    col_sources, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")\n",
    "all_sources_tot = stack_and_unit(\n",
    "    col_sources_tot, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")\n",
    "all_totals = stack_and_unit(\n",
    "    col_grand_totals, emis_units=emis_nor_unit, sector_order=sort_sector\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a3b3e6e",
   "metadata": {},
   "source": [
    "Finally we store the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e59c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sector_accounts.to_csv(output_path / \"sector_accounts.tsv\", sep=\"\\t\")\n",
    "all_findem_breakdown.to_csv(\n",
    "    output_path / \"footprints_final_demand_breakdown.tsv\", sep=\"\\t\"\n",
    ")\n",
    "all_hhld.to_csv(output_path / \"household_emissions.tsv\", sep=\"\\t\")\n",
    "all_totals.to_csv(output_path / \"total_accounts.tsv\", sep=\"\\t\")\n",
    "all_sources.to_csv(output_path / \"footprint_sources.tsv\", sep=\"\\t\")\n",
    "all_sources_tot.to_csv(output_path / \"footprint_sources_totals.tsv\", sep=\"\\t\")\n",
    "\n",
    "print(f\"Done - results stored at {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
